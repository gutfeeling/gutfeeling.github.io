{
  "createdAt": "2026-01-28T00:00:00.000000+00:00",
  "totalRecords": 6,
  "data": [
    {
      "datasetId": "v2_Public_Eval_Kaggle",
      "datasetDisplayName": "ARC-AGI-2 Public (Kaggle)",
      "modelId": "gpt-oss-120b-high",
      "displayed_on_ARC_homepage": false,
      "modelDisplayName": "GPT OSS 120b (High) Baseline CoT",
      "modelType": "CoT",
      "modelGroup": "gpt-oss-120b",
      "providerId": "Lambda (vllm)",
      "modelReleaseDate": null,
      "paperUrl": "https://link.to.our.blog.post",
      "codeUrl": "https://github.com/gutfeeling/arc-agi-2-submission/commit/d9cba8d294f102a13f7ea2e0fc0bf09ffbd5a9ba",
      "dataUrl": null,
      "score": 0.0611,
      "costPerTask": 0.23,
      "resultsUrl": null,
      "displayLabel": true,
      "modelAuthor": "OpenAI",
      "authorCategory": "AI Company",
      "baseModelFamily": "GPT OSS 120b",
      "thinkingBudgetK": null,
      "effortLevel": "High",
      "colorAuthor": "OpenAI",
      "metadata": {
        "commitId": "d9cba8d294f102a13f7ea2e0fc0bf09ffbd5a9ba",
        "experimentDescription": "Plain COT with tests passed in the prompt, 2 samples per puzzle",
        "dataPath": "evaluation/publication_final/interleaved_thinking_vs_plain_cot/gpt_oss_120b_high/baseline",
        "scoringLogic": "The two samples are used for making two submissions per puzzle. submission.json at the root of the data path (row above) contains the submissions for all puzzles. For convenience, use the scoring script in the main branch src/arcagi2/evaluation/score.py and pass the path to the submission.json file."
      }
    },
    {
      "datasetId": "v2_Public_Eval_Kaggle",
      "datasetDisplayName": "ARC-AGI-2 Public (Kaggle)",
      "modelId": "gpt-oss-120b-high",
      "displayed_on_ARC_homepage": false,
      "modelDisplayName": "GPT OSS 120b (High) (Interleaved Thinking)",
      "modelType": "Interleaved Thinking",
      "modelGroup": "gpt-oss-120b",
      "providerId": "Lambda (vllm)",
      "modelReleaseDate": null,
      "paperUrl": "https://link.to.our.blog.post",
      "codeUrl": "https://github.com/gutfeeling/arc-agi-2-submission/commit/9d5d05a4dbb70c0dab856ef18f7e3794adbd1776",
      "dataUrl": null,
      "score": 0.2639,
      "costPerTask": 0.47,
      "resultsUrl": null,
      "displayLabel": true,
      "modelAuthor": "OpenAI",
      "authorCategory": "AI Company",
      "baseModelFamily": "GPT OSS 120b",
      "thinkingBudgetK": null,
      "effortLevel": "High",
      "colorAuthor": "gutfeeling",
      "metadata": {
        "commitId": "9d5d05a4dbb70c0dab856ef18f7e3794adbd1776",
        "experimentDescription": "Interleaved thinking with tests passed in the prompt, 2 samples per puzzle",
        "dataPath": "evaluation/publication_final/interleaved_thinking_vs_plain_cot/gpt_oss_120b_high/interleaved_thinking",
        "scoringLogic": "The two samples are used for making two submissions per puzzle. submission.json at the root of the data path (row above) contains the submissions for all puzzles. For convenience, use the scoring script in the main branch src/arcagi2/evaluation/score.py and pass the path to the submission.json file."
      }
    },
    {
      "datasetId": "v2_Public_Eval_Kaggle",
      "datasetDisplayName": "ARC-AGI-2 Public (Kaggle)",
      "modelId": "MiniMax-M2.1",
      "displayed_on_ARC_homepage": false,
      "modelDisplayName": "Minimax M2.1 Baseline CoT",
      "modelType": "CoT",
      "modelGroup": "MiniMax-M2",
      "providerId": "Minimax",
      "modelReleaseDate": null,
      "paperUrl": "https://link.to.our.blog.post",
      "codeUrl": "https://github.com/gutfeeling/arc-agi-2-submission/commit/9d5d05a4dbb70c0dab856ef18f7e3794adbd1776",
      "dataUrl": null,
      "score": 0.0306,
      "costPerTask": 0.22,
      "resultsUrl": null,
      "displayLabel": true,
      "modelAuthor": "Minimax",
      "authorCategory": "AI Company",
      "baseModelFamily": "Minimax M2.1",
      "thinkingBudgetK": null,
      "effortLevel": null,
      "colorAuthor": "Custom",
      "metadata": {
        "commitId": "9d5d05a4dbb70c0dab856ef18f7e3794adbd1776",
        "experimentDescription": "Plain COT with tests passed in the prompt, 2 samples per puzzle",
        "dataPath": "evaluation/publication_final/interleaved_thinking_vs_plain_cot/minimax_m2_1/baseline",
        "scoringLogic": "The two samples are used for making two submissions per puzzle. submission.json at the root of the data path (row above) contains the submissions for all puzzles. For convenience, use the scoring script in the main branch src/arcagi2/evaluation/score.py and pass the path to the submission.json file."
      }
    },
    {
      "datasetId": "v2_Public_Eval_Kaggle",
      "datasetDisplayName": "ARC-AGI-2 Public (Kaggle)",
      "modelId": "MiniMax-M2.1",
      "displayed_on_ARC_homepage": false,
      "modelDisplayName": "Minimax M2.1 (Interleaved Thinking)",
      "modelType": "Interleaved Thinking",
      "modelGroup": "MiniMax-M2",
      "providerId": "Minimax",
      "modelReleaseDate": null,
      "paperUrl": "https://link.to.our.blog.post",
      "codeUrl": "https://github.com/gutfeeling/arc-agi-2-submission/commit/9d5d05a4dbb70c0dab856ef18f7e3794adbd1776",
      "dataUrl": null,
      "score": 0.1056,
      "costPerTask": 1.27,
      "resultsUrl": null,
      "displayLabel": true,
      "modelAuthor": "Minimax",
      "authorCategory": "Unknown",
      "baseModelFamily": "Minimax M2.1",
      "thinkingBudgetK": null,
      "effortLevel": null,
      "colorAuthor": "gutfeeling",
      "metadata": {
        "commitId": "9d5d05a4dbb70c0dab856ef18f7e3794adbd1776",
        "experimentDescription": "Interleaved thinking with tests passed in the prompt, 2 samples per puzzle",
        "dataPath": "evaluation/publication_final/interleaved_thinking_vs_plain_cot/minimax_m2_1/interleaved_thinking",
        "scoringLogic": "The two samples are used for making two submissions per puzzle. submission.json at the root of the data path (row above) contains the submissions for all puzzles. For convenience, use the scoring script in the main branch src/arcagi2/evaluation/score.py and pass the path to the submission.json file."
      }
    },
    {
      "datasetId": "v2_Public_Eval_Kaggle",
      "datasetDisplayName": "ARC-AGI-2 Public (Kaggle)",
      "modelId": "gpt-5-2-2025-12-11-thinking-xhigh",
      "displayed_on_ARC_homepage": false,
      "modelDisplayName": "GPT 5.2 XHigh Baseline CoT (107 puzzles)",
      "modelType": "CoT",
      "modelGroup": "gpt-5-2-2025-12-11-thinking",
      "providerId": "OpenAI",
      "modelReleaseDate": null,
      "paperUrl": "https://link.to.our.blog.post",
      "codeUrl": "https://huggingface.co/datasets/arcprize/arc_agi_v2_public_eval/tree/main/gpt-5-2-2025-12-11-thinking-xhigh",
      "dataUrl": "https://huggingface.co/datasets/arcprize/arc_agi_v2_public_eval/tree/main/gpt-5-2-2025-12-11-thinking-xhigh",
      "score": 0.5981,
      "costPerTask": 2.05,
      "resultsUrl": null,
      "displayLabel": true,
      "modelAuthor": "OpenAI",
      "authorCategory": "AI Company",
      "baseModelFamily": "GPT 5.2",
      "thinkingBudgetK": null,
      "effortLevel": "X-High",
      "colorAuthor": "OpenAI",
      "metadata": {
        "commitId": "N/A",
        "experimentDescription": "Plain COT with tests passed in the prompt, 2 samples per puzzle",
        "dataPath": "https://huggingface.co/datasets/arcprize/arc_agi_v2_public_eval/tree/main/gpt-5-2-2025-12-11-thinking-xhigh",
        "scoringLogic": "Only 107 puzzles scored. 59.81% if scoring on 107 puzzles, otherwise 53.33% on the full set"
      }
    },
    {
      "datasetId": "v2_Public_Eval_Kaggle",
      "datasetDisplayName": "ARC-AGI-2 Public (Kaggle)",
      "modelId": "gpt-5-2-2025-12-11-thinking-xhigh",
      "displayed_on_ARC_homepage": false,
      "modelDisplayName": "GPT 5.2 XHigh (Interleaved Thinking) (107 puzzles)",
      "modelType": "Interleaved Thinking",
      "modelGroup": "gpt-5-2-2025-12-11-thinking",
      "providerId": "OpenAI",
      "modelReleaseDate": null,
      "paperUrl": "https://link.to.our.blog.post",
      "codeUrl": "https://github.com/gutfeeling/arc-agi-2-submission/commit/9d5d05a4dbb70c0dab856ef18f7e3794adbd1776",
      "dataUrl": null,
      "score": 0.7336,
      "costPerTask": 4.35,
      "resultsUrl": null,
      "displayLabel": true,
      "modelAuthor": "OpenAI",
      "authorCategory": "Independent",
      "baseModelFamily": "GPT 5.2",
      "thinkingBudgetK": null,
      "effortLevel": "X-High",
      "colorAuthor": "gutfeeling",
      "metadata": {
        "commitId": "9d5d05a4dbb70c0dab856ef18f7e3794adbd1776",
        "experimentDescription": "Interleaved thinking with tests passed in the prompt, 2 samples per puzzle",
        "dataPath": "evaluation/publication_final/interleaved_thinking_vs_plain_cot/gpt_5_2_xhigh/interleaved_thinking",
        "scoringLogic": "The two samples are used for making two submissions per puzzle. submission.json at the root of the data path (row above) contains the submissions for all puzzles. For convenience, use the scoring script in the main branch src/arcagi2/evaluation/score.py and pass the path to the submission.json file. 72.43% if scoring on 107 puzzles, otherwise 67.5%"
      }
    }
  ]
}
