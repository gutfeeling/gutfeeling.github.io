[
  {
    "objectID": "posts/what-kind-of-ai-would-you-like/index.html",
    "href": "posts/what-kind-of-ai-would-you-like/index.html",
    "title": "What kind of AI would you like?",
    "section": "",
    "text": "As the organizer of an AGI focused meetup group in my city, I regularly get to talk to a bunch of informed people about their perspective on frontier models and AI systems. What is getting clearer to me is that capability gaps between present day SOTA and the goalpost (AGI) is probably not the most useful framing for talking about the topic.\nThe problem with this framing is that the goalpost is quite subjective. I am yet to meet two people who had the exact same definition of AGI. There are some people, including highly accomplished individuals like Peter Norvig, who think that we already have AGI. This actually seems like a perfectly reasonable take to me! You can talk to frontier models about any topic, and they are already better than your average Joe in many domains. Of course, there are still capability gaps - things that are easy or possible for humans to do, but hard or impossible for machines to do. But turning that argument on its head, there are capability gaps in the other direction too. There are things that are easy or possible for machines to do, but hard or impossible for humans. For example, no human is knowledgeable about every domain under the sun, while frontier models are broadly knowledgeable about almost everything.\nThis essentially brings us to the crux of the matter. Human intelligence and present day frontier models might just be two different forms of intelligence with different characteristics, and it might be pretty futile to order them in the mathematical sense. Closing capability gaps will still remain interesting for scientific and economic reasons, but philosophically, that’s perhaps not the interesting thing anymore.\nThere’s something that Geoffrey Hinton said recently that I find quite interesting and relevant to this discussion. Hinton has famously spent many years trying to come up with new architectures and training methods that better resemble what the human brain does. Recently, he admitted that he may have gotten it wrong. He had indulged in the fallacy that human-like intelligence is the only reasonable target, and therefore, trying to match the human brain is a safe bet. What he had ignored is that there can be many types of general intelligences, just like there are many types of databases. And just like we have the CAP theorem in distributed data systems, which states that one cannot have data consistency, availability, and partition tolerance simultaneously in any system, similarly general intelligence may also be a tradeoff space, with the human brain occupying a narrow spot in the possible spectrum.\nSpecifically, he said that such a fundamental tradeoff might exist for energy efficiency and copy efficiency.\n\nThe human brain is clearly more energy efficient, using 10 - 20 watts, whereas today’s frontier models use several order of magnitudes more.\nFrontier models are clearly more copy efficient. Weights can be downloaded instantly from Huggingface with a high bandwidth internet connection, or copied across different models via distillation. On the other hand, the classical way to transfer knowledge from one brain to another is teaching, which is a low bandwidth and lossy process.\n\nTaking inspiration from Hinton’s thesis, I have been thinking about whether there could be a similar tradeoff between continual learning and domain-generality. Will AI with human-like continual learning naturally be human-like specialists? I want to elaborate on this in another essay, but I just wanted to mention this as another speculative tradeoff.\nMore and more, this seems to be the useful way of thinking about general intelligence. To summarize the framing:\n\nThere are many different types of general intelligence.\nGeneral intelligence is essentially a design space with potentially fundamental tradeoffs.\nEven if we assume that the intelligence that is currently being scaled is a form of general intelligence, one shouldn’t think of it as the “one and the only one” form that’s possible or even preferred.\n\nIf this is true, then it immediately begs the following question:\n\nIs the type of intelligence that’s being currently scaled the one we want? What kind of artificial general intellience would you want?\n\nIn my opinion, this is the more important question than whether current frontier models are AGI or whether AGI is coming in N years.\nIt’s a very hard question because there are no right answers and no obvious benchmark to guide progress. It’s more in the domain of art and design than science, though science will be needed to make ambitious imaginations and dreams possible."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a software engineer from Munich, Germany. Welcome to my blog!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dibya’s Blog",
    "section": "",
    "text": "What kind of AI would you like?\n\n\n\nai\n\nopinion\n\n\n\n\"When AGI?\" is not the interesting question\n\n\n\n\n\nFeb 26, 2026\n\n\nDibya Chakravorty\n\n\n\n\n\n\n\n\n\n\n\n\nOur ARC AGI 2 journey\n\n\n\nai\n\narc-agi\n\n\n\nAn independent team working in their free time can make meaningful research progress\n\n\n\n\n\nFeb 24, 2026\n\n\nDibya Chakravorty\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/arc-agi-2/index.html",
    "href": "posts/arc-agi-2/index.html",
    "title": "Our ARC AGI 2 journey",
    "section": "",
    "text": "My friends and I recently obtained a simple but surprising result on ARC AGI 2, getting &gt; 4x performance improvement from GPT OSS 120B and double digit gains in GPT 5.2. Our team’s blog post describes the work in formal detail, while I will record our journey here.\nMy friends and I casually started working on ARC AGI 2 last summer, with the goal of participating in the ARC Prize Kaggle competition. Early on, we were exploring agentic coding with frontier reasoning models and found that models like o3 and o4-mini could generate high-quality synthetic ARC-style puzzles.\nWe generated a dataset containing ~ 8000 high-quality synthetic puzzles of varying complexity. Here are a few illustrative examples.\n\n\n\nA standard synthetic puzzle\n\n\n\n\n\nA synthetic puzzle requiring contextual rule application\n\n\n\n\n\nA synthetic puzzle requiring multi-step composition\n\n\nWe planned to use these synthetic puzzles to train a smaller model via agentic reinforcement learning (RLVR with interleaved thinking).\nWe wanted to bootstrap training by distilling on successful solution traces from an open-weight reasoning model. That requirement led us to investigate GPT-OSS-120B. Initially, we were disappointed since we weren’t able to reliably elicit interleaved thinking from the model, no matter whether we used inference providers on Openrouter or self-hosted solutions like vLLM and SGLang. This led us on a journey of investigating how vLLM and SGLang implements the chat template for the model. We found that they are buggy, and patched vLLM to fix the bug.\nAt this point, we noticed something unexpected: simply placing the model into an agentic coding regime produced large and consistent score improvements on the ARC AGI public eval. We are talking about &gt; 4x improvement relative to plain COT. We couldn’t believe the scores we were getting from a medium sized OSS model!\nThis observation ultimately shifted the focus of our work as we wanted to find out how universally this observation applies. We tested three model families and got positive results on all three. At this point, we decided to publish our results.\n\n\n\nAccuracy on the ARC AGI 2 public eval set\n\n\nShortly afterwards, a neolab called Symbolica announced SOTA applying the same method with the newly released Claude Opus 4.6. Here’s their post from X.\nWe set a new ARC-AGI-2 SotA: 85.28% using an Agentica agent (~350 lines) that writes and runs code. pic.twitter.com/tohFfBZb2P— Agentica (@agenticasdk) February 12, 2026 \nA few weeks later, a YCombinator startup Confluence Labs saturated the ARC AGI 2 public eval (97.9%) using the same method with the newly released Gemini 3.1 Pro. Here’s their post from X.\n.@_confluencelabs is coming out of stealth with SOTA on ARC-AGI-2 (97.9%).They're focused on learning efficiency — making AI useful where data is sparse and experiments are costly. Read more at https://t.co/K9NEFR6M0SCongrats on the launch, @BingBongBrent and @bankminer78!… pic.twitter.com/4VjDyPNfvP— Y Combinator (@ycombinator) February 24, 2026 \nOther than clear implications on SOTA, I think this raises interesting scientific questions.\n\nAll the 5 model families tested by our group, Symbolica and Confluence Labs had a significant agentic RL posttraining phase. Since our method performs inference under the exact same condition that was available during agentic RL training, does the capability jump indicate that agentic RL training induces additional fluid intelligence in models?\nThere is a widespread belief that modern frontier models are trained on massive amounts of synthetic ARC AGI 2 like puzzles. This is termed “benchmaxxing” and many people argue that the performance increases observed in the leaderboard aren’t evidence of true fluid intelligence. Our results on open-weight models could provide a counterpoint. The plain COT performance of the open-weight models are at near noise levels (~ 5% on ARC AGI 2), effectively ruling out benchmaxxing. Interestingly, the performance of the same models jump significantly in the inference regime corresponding to agentic RL training. Could this be indicative of true increases in fluid intelligence?"
  }
]